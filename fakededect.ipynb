{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b3c8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1540792444.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\anmol\\AppData\\Local\\Temp\\ipykernel_11864\\1540792444.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    pip install --upgrade scikit-learn==1.2.2\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn==1.2.2\n",
    "!pip install numpy pandas nltk matplotlib ipywidgets imbalanced-learn shap requests\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import joblib\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.calibration import CalibrationDisplay, CalibratedClassifierCV\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import shap\n",
    "\n",
    "# 2. NLTK Resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# 3. Data Loading and Validation\n",
    "def validate_data(df):\n",
    "    if 'text' not in df.columns:\n",
    "        raise ValueError(\"Dataset missing 'text' column\")\n",
    "    if df['text'].isnull().any():\n",
    "        raise ValueError(\"Dataset contains missing text values\")\n",
    "    return df\n",
    "\n",
    "df_fake = validate_data(pd.read_csv(\"Fake.csv\"))\n",
    "df_real = validate_data(pd.read_csv(\"True.csv\"))\n",
    "df_fake[\"label\"] = 0  # Fake\n",
    "df_real[\"label\"] = 1  # Real\n",
    "df = pd.concat([df_fake, df_real], ignore_index=True).sample(frac=1, random_state=42)\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df = df[df['date'] > '2010-01-01']\n",
    "\n",
    "# 4. Enhanced Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,;:!?\\'\" ]', '', str(text), flags=re.I).lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens \n",
    "                     if word not in stop_words and len(word) > 2])\n",
    "\n",
    "df[\"cleaned_text\"] = df[\"text\"].apply(preprocess)\n",
    "\n",
    "# 5. Feature Engineering and Train/Test Split\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "X = tfidf.fit_transform(df[\"cleaned_text\"])\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 6. Class Balancing on Training Data\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_bal, y_train_bal = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# 7. Custom Logistic Regression\n",
    "class BalancedLogisticRegression(LogisticRegression):\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        if sample_weight is None:\n",
    "            class_weights = {0: len(y)/sum(y==0), 1: len(y)/sum(y==1)}\n",
    "            sample_weight = [class_weights[c] for c in y]\n",
    "        return super().fit(X, y, sample_weight)\n",
    "\n",
    "# 8. Hyperparameter Tuning\n",
    "lr_params = {'C': [0.1, 1, 10], 'class_weight': [None, 'balanced']}\n",
    "lr = GridSearchCV(\n",
    "    BalancedLogisticRegression(max_iter=1000),\n",
    "    lr_params,\n",
    "    cv=3,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "lr.fit(X_train_bal, y_train_bal)\n",
    "best_model = lr.best_estimator_\n",
    "\n",
    "# 9. Model Calibration\n",
    "calibrated_model = CalibratedClassifierCV(best_model, cv='prefit')\n",
    "calibrated_model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# 10. Save Models\n",
    "joblib.dump(calibrated_model, \"news_model.pkl\")\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# 11. Evaluation\n",
    "print(\"=== Model Performance ===\")\n",
    "print(f\"Best Parameters: {lr.best_params_}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, calibrated_model.predict(X_test)):.2%}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, calibrated_model.predict(X_test)):.2%}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,6))\n",
    "ConfusionMatrixDisplay.from_estimator(calibrated_model, X_test, y_test, ax=ax1)\n",
    "ax1.set_title(\"Confusion Matrix\")\n",
    "CalibrationDisplay.from_estimator(\n",
    "    calibrated_model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    n_bins=10,\n",
    "    ax=ax2\n",
    ")\n",
    "ax2.set_title(\"Calibration Curve\")\n",
    "plt.show()\n",
    "\n",
    "# 12. Fact-Checking API Integration\n",
    "API_KEY = 'add your google api'  # <-- Replace with your actual API key\n",
    "API_ENDPOINT = \"https://factchecktools.googleapis.com/v1alpha1/claims:search\"\n",
    "\n",
    "def check_fact_claims(text):\n",
    "    params = {'query': text, 'key': API_KEY}\n",
    "    try:\n",
    "        response = requests.get(API_ENDPOINT, params=params)\n",
    "        response.raise_for_status()\n",
    "        claims = response.json().get('claims', [])\n",
    "        return claims if claims else None\n",
    "    except Exception as e:\n",
    "        return f\"API Error: {str(e)}\"\n",
    "\n",
    "def get_source_credibility(text):\n",
    "    verified_sources = ['reuters', 'associated press', 'bbc']\n",
    "    return \"Verified Source\" if any(source in text.lower() for source in verified_sources) else \"Unverified Source\"\n",
    "\n",
    "# 13. Interactive Interface\n",
    "input_text = widgets.Textarea(\n",
    "    placeholder='Paste news article here...',\n",
    "    layout={'width': '90%', 'height': '200px'}\n",
    ")\n",
    "analyze_btn = widgets.Button(\n",
    "    description=\"Analyze Article\", \n",
    "    button_style='success',\n",
    "    layout={'width': '200px'}\n",
    ")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_analyze_click(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        text = input_text.value.strip()\n",
    "        if not text:\n",
    "            print(\"Error: Please enter a news article!\")\n",
    "            return\n",
    "\n",
    "        source_status = get_source_credibility(text)\n",
    "        clean_text = preprocess(text)\n",
    "        vector = tfidf.transform([clean_text])\n",
    "        pred = calibrated_model.predict(vector)[0]\n",
    "        proba = calibrated_model.predict_proba(vector)[0][1]\n",
    "\n",
    "        fact_check = check_fact_claims(text)\n",
    "        print(f\"\\n{'‚ïê'*60}\")\n",
    "        print(f\"üì∞ Source: {source_status}\")\n",
    "        print(f\"üîç Prediction: {'FAKE NEWS üî¥' if pred == 0 else 'REAL NEWS üü¢'}\")\n",
    "        print(f\"üìä Confidence: {proba*100:.2f}%\")\n",
    "        print(f\"{'‚ïê'*60}\\n\")\n",
    "\n",
    "        print(\"üß† Top Predictive Features:\")\n",
    "        feature_names = tfidf.get_feature_names_out()\n",
    "        coefs = calibrated_model.base_estimator.coef_[0]\n",
    "        top_features = sorted(zip(feature_names, coefs), key=lambda x: abs(x[1]), reverse=True)[:5]\n",
    "        for feat, weight in top_features:\n",
    "            print(f\" - {'üö©' if weight < 0 else '‚úÖ'} {feat}: {abs(weight):.2f}\")\n",
    "\n",
    "        print(\"\\nüîç Prediction Explanation:\")\n",
    "        explainer = shap.LinearExplainer(calibrated_model.base_estimator, X_train_bal)\n",
    "        shap_values = explainer.shap_values(vector)\n",
    "        shap.plots.waterfall(shap_values[0], max_display=10,show=False)\n",
    "\n",
    "        print(\"\\nüîé Fact-Check Findings:\")\n",
    "        if isinstance(fact_check, str):\n",
    "            print(fact_check)\n",
    "        elif fact_check:\n",
    "            for claim in fact_check[:3]:\n",
    "                print(f\"- Claim: {claim.get('text', 'N/A')}\")\n",
    "                print(f\"  By: {claim.get('claimant', 'Unknown')}\")\n",
    "                if claim.get('claimReview'):\n",
    "                    review = claim['claimReview'][0]\n",
    "                    print(f\"  Verdict: {review.get('textualRating', 'N/A')}\")\n",
    "                    print(f\"  Source: {review.get('publisher', {}).get('name', 'N/A')}\")\n",
    "                    print(f\"  URL: {review.get('url', 'N/A')}\\n\")\n",
    "        else:\n",
    "            print(\"No related fact-checks found\")\n",
    "\n",
    "analyze_btn.on_click(on_analyze_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"\"\"\n",
    "        <h1 style='text-align:center'>üïµÔ∏è‚ôÇÔ∏è Fake News Detector</h1>\n",
    "        <h3 style='text-align:center'>AI-Powered Verification with Fact-Checking</h3>\n",
    "    \"\"\"),\n",
    "    widgets.HTML(\"<b>Enter News Article:</b>\"),\n",
    "    input_text,\n",
    "    widgets.HBox([analyze_btn], layout={'justify_content': 'center'}),\n",
    "    output\n",
    "]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
